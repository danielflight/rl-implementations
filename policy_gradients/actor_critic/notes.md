These follow approx. policy gradients

- Consider the following:
  - Q Actor-Critic
  - Advantage Actor-Critic (Q-V)
  - TD Actor-Critic (using a sample of the TD target as advantage func)
  - TD(lambda) Actor-Critic 
  
- CONSIDER Deterministic Policy Gradients?



--- USE A HARDER ENV (CONT ACTIONS) OR USE CARTPOLE AND COMPARE TO REINFORCE