{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6058050a",
   "metadata": {},
   "source": [
    "# Model-Based Reinforcement Learning\n",
    "\n",
    "Model-based reinforcement learning (MBRL) aims to improve sample efficiency by learning a **model of the environment’s dynamics** and using it for **planning** or **simulated experience**.  \n",
    "Instead of learning purely from environment interactions, the agent leverages its learned model to reason about future outcomes.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 1. Simple Model-Based RL (Sample-Based Planning)\n",
    "\n",
    "Train a neural model to predict:\n",
    "- The **next state**: $\\hat{s}' = f_\\theta(s, a)$\n",
    "- The **reward**: $\\hat{r} = g_\\phi(s, a)$\n",
    "\n",
    "Once the model approximates the environment’s dynamics well, the agent can:\n",
    "- **Sample transitions** from the model: $(s, a, \\hat{r}, \\hat{s}')$\n",
    "- **Train a model-free RL algorithm** (e.g. DQN, A2C) using these simulated transitions.\n",
    "\n",
    "### Example Workflow\n",
    "1. Collect real transitions $(s, a, r, s')$ from the environment.  \n",
    "2. Train neural networks $f_\\theta$ and $g_\\phi$ to predict $s'$ and $r$.  \n",
    "3. Use the model to simulate new transitions.  \n",
    "4. Train a policy/value function on this generated data.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Dyna-Q (Combined Learning and Planning)\n",
    "\n",
    "**Dyna-Q** (Sutton, 1990) is a **hybrid** approach that combines:\n",
    "- **Model-free RL** (learning from real experience)\n",
    "- **Model-based planning** (learning from simulated experience)\n",
    "\n",
    "After each real environment step:\n",
    "1. Update the **Q-function** using the real transition $(s, a, r, s')$.\n",
    "2. Update the **learned model** $\\hat{P}, \\hat{R}$ using the same data.\n",
    "3. **Sample “imaginary” experiences** from the model and perform additional Q-learning updates.\n",
    "\n",
    "This allows the agent to “replay” and reinforce knowledge without re-interacting with the environment.\n",
    "\n",
    "### Algorithm\n",
    "\n",
    "\n",
    "**for** each real step (s, a, r, s'):\n",
    "<br>\n",
    "<br> update $Q(s,a)$\n",
    "<br> update $model(\\hat{R}$, $\\hat{P}$)\n",
    "\n",
    "   **for** $k$ simulated steps:\n",
    "   1. sample ($s_{sim}, a_{sim}$) from memory: \n",
    "   2. $s'_{sim}$, $r_{sim}$ = $model(s_{sim}$, $a_{sim})$\n",
    "   3. update $Q(s_{sim}, a_{sim})$\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Monte Carlo Tree Search (MCTS) and Monte Carlo Planning\n",
    "\n",
    "Rather than sampling random one-step transitions, **Monte Carlo Tree Search (MCTS)** performs **lookahead planning** by simulating multiple future trajectories from a given state using the learned model.\n",
    "\n",
    "The agent builds a **search tree**:\n",
    "- Nodes represent states.\n",
    "- Edges represent actions.\n",
    "- Values are estimated via simulated rollouts.\n",
    "\n",
    "This allows more strategic planning and decision-making at each step.\n",
    "\n",
    "- MCTS can act as the **planning component** in Dyna-Q.\n",
    "- Instead of sampling one-step transitions, the agent performs **multi-step rollouts** using the model to estimate expected returns for each action.\n",
    "- The results are used to update Q-values or guide policy selection.\n",
    "\n",
    "### Advantages\n",
    "- Enables deeper lookahead than simple Dyna sampling.\n",
    "- Reduces variance in value estimates.\n",
    "- Strong theoretical and practical foundation (e.g. AlphaZero).\n",
    "\n",
    "### Challenges\n",
    "- Computationally expensive.\n",
    "- Requires a well-calibrated model for reliable rollouts.\n",
    "\n",
    "---\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Approach | Uses Learned Model? | Uses Real Env? | Planning Style | Pros | Cons |\n",
    "|-----------|--------------------|----------------|----------------|------|------|\n",
    "| **Simple Model-Based RL** | ✅ | ❌ | Sampling synthetic transitions | High efficiency | Model bias |\n",
    "| **Dyna-Q** | ✅ | ✅ | Sampled replay (mix of real + model) | Balanced learning | Complex trade-off |\n",
    "| **MCTS / MC Search** | ✅ | Optional | Tree-based rollouts | Strategic planning | High compute cost |\n",
    "\n",
    "---\n",
    "\n",
    "### References\n",
    "- Sutton & Barto (1998): *Reinforcement Learning: An Introduction* — Ch. 8 (Planning and Learning)\n",
    "- Sutton (1990): *Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming*\n",
    "- Silver et al. (2017): *Mastering the Game of Go without Human Knowledge (AlphaZero)*\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
